---
title: "Homework 4 p2 v1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r read in data}
#read in "gauge.txt" into data
data <- read.table("gauge.txt", header=TRUE)
head(data)
```

## 2.
<b>Predicting:</b> 
Ultimately we are interested in answering questions such as: Given a gain reading of 38.6, what is the density of the snow-pack? or Given a gain reading of 426.7, what is the density of snow-pack? These two numeric values, 38.6 and 426.7, were chosen because they are the average gains for the 0.508 and 0.001 densities, respectively.

** Develop a procedure for adding CIs around your least squares line that can be used to make interval estimates for the snow-pack density from gain measurements. Keep in mind how the data were collected: several measurements of gain were taken for polyenthylene blocks of known density.

```{r}
gan = data$gain
den = data$density
plot(gan, den, pch=16)
```

In such case, our remedy is polynomial regression. To start, we will first look at a degree 1 and 2 examples.
```{r}
fit.d1 <- lm(den~gan)
fit.d2 <- lm(den~poly(gan, 2, raw=TRUE))
pts <- seq(0, 600, length.out=100)
val.d1 <- predict(fit.d1, data.frame(gan=pts))
val.d1 
val.d2 <- predict(fit.d2, data.frame(gan=pts))
plot(gan, den, pch=16)
lines(pts, val.d1, col="blue", lwd=2)
lines(pts, val.d2, col="red", lwd=2)
```

Below shows the confidence intervals and prediction intervals based on degree 1 and degree 2 polynomial regression. The black lines are fitted lines, the blue lines are the pointwise confidence intervals and the red lines are the prediction intervals.
```{r}
CI.conf <- predict(fit.d1, data.frame(gan=pts), interval = "confidence") #confidence interval
CI.pred <- predict(fit.d1, data.frame(gan=pts), interval = "predict") #prediction interval
plot(gan, den, pch=16)
lines(pts, CI.conf[,"fit"], col="black", lwd=2)
lines(pts, CI.conf[,"lwr"], col="blue", lwd=1) 
lines(pts, CI.conf[,"upr"], col="blue", lwd=1)
lines(pts, CI.pred[,"lwr"], col="red", lwd=1)
lines(pts, CI.pred[,"upr"], col="red", lwd=1)
```


```{r}
CI.conf <- predict(fit.d2, data.frame(gan=pts), interval = "confidence") #confidence interval
CI.pred <- predict(fit.d2, data.frame(gan=pts), interval = "predict") #prediction interval
plot(gan, den, pch=16)
lines(pts, CI.conf[,"fit"], col="black", lwd=2)
lines(pts, CI.conf[,"lwr"], col="blue", lwd=1) 
lines(pts, CI.conf[,"upr"], col="blue", lwd=1)
lines(pts, CI.pred[,"lwr"], col="red", lwd=1)
lines(pts, CI.pred[,"upr"], col="red", lwd=1)
```

The degree 1 polynomial fit is exactly the same linear regression. The degree 2 polynomial is a quandratic fit to the data. The quadratic fit is slightly better one can argue. In fact, we can verify the claim using R2 of the 2 fits.
```{r}
summary(fit.d1)$r.squared

summary(fit.d2)$r.squared
```

However, as we talked about earlier, R2 is not a good measure, when we compare two models of different model complexity. In fact, if one increases the degree of polynomial, R2 increases as well.
```{r}
d.max = 30
R.square = rep(NA, d.max)
for (d in 1:d.max){
    fit = lm(den ~ poly(gan, d, raw=TRUE))
    R.square[d] = summary(fit)$r.squared
}
plot(1:d.max, R.square, xlab = "Degree", ylab = "R-squared", lwd = 2, col = "blue", pch = 5)
lines(1:d.max, R.square, type='l', lwd = 2, col = "blue")
```

We can also take a look at the actual fits of the polynomials ranging from degree 1 to 10.
```{r}
plot(gan, den, pch = 16)
pts <- seq(0, 600, length.out=100)
for (d in 1:10){
    fit <- lm(den ~ poly(gan, d, raw=TRUE))
    val <- predict(fit, data.frame(gan = pts))
    lines(pts, val, col=rainbow(10)[d], lwd = 2)
}
```

(NOT MINE DETAILS MAY BE WRONG)

Obviously, this shows that even though polynomial models of higher degree return larger R2 for the fit, the higher degree model will overfit the data. One can see that the high degree polynomials are very volatile. It does perform well in fitting the data we have at hand, but it will make large errors in predicting data not currently in the dataset.

So how do we choose the degree of our polynomial regression? A common approach is to stop at a low degree which R2 does not increase much further. For example, from our example, we see that R2 increase from about 0.68 to over 0.72 at degree 2, which is actually the biggest single step gain. But then one might also argue that degree 8 has achieved 0.76 in R2. If we take a look at degree 8 polynomial below, we can see that it is apparently overfitting.
```{r}
plot(gan, den, pch = 16)
pts <- seq(0, 600, length.out=100)
fit.d8 <- lm(den ~ poly(gan, 8, raw=TRUE))
val.d8 <- predict(fit.d8, data.frame(gan = pts))
lines(pts, val.d8, col="red", lwd = 2)
```

## NEW

```{r}
#read in "gauge.txt" into data
data <- read.table("gauge.txt", header=TRUE)
head(data)
```

## 2.
<b>Predicting:</b> 
Ultimately we are interested in answering questions such as: Given a gain reading of 38.6, what is the density of the snow-pack? or Given a gain reading of 426.7, what is the density of snow-pack? These two numeric values, 38.6 and 426.7, were chosen because they are the average gains for the 0.508 and 0.001 densities, respectively.

** Develop a procedure for adding CIs around your least squares line that can be used to make interval estimates for the snow-pack density from gain measurements. Keep in mind how the data were collected: several measurements of gain were taken for polyenthylene blocks of known density.

```{r}
gain = data$gain
density = data$density
plot(gain, density, pch=16)
```

```{r}
log.gain = log(data$gain)
lm2 = lm(density~log.gain, data = data)
summary(lm2)
```

In such case, our remedy is polynomial regression. To start, we will first look at a degree 1 and 2 examples.
```{r}
fit.d1 <- lm(density~log.gain, data = data)
pts <- seq(0, 600, length.out=90)
val.d1 <- predict(fit.d1, data.frame(log.gain=pts))
plot(log.gain, density, pch=16)
lines(pts, val.d1, col="blue", lwd=2)
```

```{r}
CI.conf <- predict(fit.d1, data.frame(log.gain=pts), interval = "confidence") #confidence interval
plot(log.gain, density, pch=16)
lines(pts, CI.conf[,"fit"], col="black", lwd=2)
lines(pts, CI.conf[,"lwr"], col="blue", lwd=1) 
lines(pts, CI.conf[,"upr"], col="blue", lwd=1)
```

```{r}
fit.d1 <- lm(log.gain~density, data = data)
pts <- seq(-10, 600, length.out=90)
val.d1 <- predict(fit.d1, data.frame(density=pts))
plot(log.gain, density, pch=16)
lines(pts, val.d1, col="blue", lwd=2)
```

```{r}
CI.conf <- predict(fit.d1, data.frame(density=pts), interval = "confidence") #confidence interval
plot(density, log.gain, pch=16)
lines(pts, CI.conf[,"fit"], col="black", lwd=2)
lines(pts, CI.conf[,"lwr"], col="blue", lwd=1) 
lines(pts, CI.conf[,"upr"], col="blue", lwd=1)
```

```{r}
fit.d1 <- lm(log.gain~density, data = data)
pts <- seq(-600, 600, length.out=90)
val.d1 <- predict(fit.d1, data.frame(density=pts))
plot(log.gain, density, pch=16)
lines(pts, val.d1, col="blue", lwd=2)
```

```{r}
CI.conf <- predict(fit.d1, data.frame(density=pts), interval = "confidence") #confidence interval
plot(density, log.gain, pch=16)
lines(pts, CI.conf[,"fit"], col="black", lwd=2)
lines(pts, CI.conf[,"lwr"], col="blue", lwd=1) 
lines(pts, CI.conf[,"upr"], col="blue", lwd=1)
```

```{r}
fit.d1 <- lm(density~log.gain, data = data)
pts <- seq(0, 8, length.out=90)
val.d1 <- predict(fit.d1, data.frame(log.gain=pts))
plot(log.gain, density, pch=16)
lines(pts, val.d1, col="blue", lwd=2)
```

```{r}
CI.conf <- predict(fit.d1, data.frame(log.gain=pts), interval = "confidence") #confidence interval
CI.pred <- predict(fit.d1, data.frame(log.gain=pts), interval = "predict") #prediction interval
plot(log.gain, density, pch=16, xlab='Log(Gain)', ylab='Density', main='95% Confidence Interval')
#lines(pts, CI.conf[,"fit"], col="black", lwd=2)
lines(pts, CI.conf[,"lwr"], col="blue", lwd=1) 
lines(pts, CI.conf[,"upr"], col="blue", lwd=1)

lines(pts, CI.pred[,"lwr"], col="red", lwd=1)
lines(pts, CI.pred[,"upr"], col="red", lwd=1)
```

```{r}
newdata = data.frame(density=0.6860)
predict(fit.d1, newdata, interval = "confidence")
```

```{r}
fit.d1 <- lm(log.gain~density, data = data)
pts <- seq(0, 8, length.out=90)
val.d1 <- predict(fit.d1, data.frame(density=pts))
plot(density, log.gain, pch=16)
lines(pts, val.d1, col="blue", lwd=2)
```

```{r}
CI.conf <- predict(fit.d1, data.frame(density=pts), interval = "confidence") #confidence interval
CI.pred <- predict(fit.d1, data.frame(density=pts), interval = "predict") #prediction interval
plot(density, log.gain, pch=16, xlab='Density', ylab='Log(Gain)', main='95% Confidence Interval')
#lines(pts, CI.conf[,"fit"], col="black", lwd=2)
lines(pts, CI.conf[,"lwr"], col="blue", lwd=1) 
lines(pts, CI.conf[,"upr"], col="blue", lwd=1)

lines(pts, CI.pred[,"lwr"], col="red", lwd=1)
lines(pts, CI.pred[,"upr"], col="red", lwd=1)
```

```{r}
newdata = data.frame(density=0.6860)
predict(fit.d1, newdata, interval = "confidence")
```

```{r}
newdata = data.frame(density=0.6040)
predict(fit.d1, newdata, interval = "confidence")
```

```{r}
newdata = data.frame(density=0.5080)
predict(fit.d1, newdata, interval = "confidence")
```

```{r}
newdata = data.frame(density=0.4120)
predict(fit.d1, newdata, interval = "confidence")
```

```{r}
newdata = data.frame(density=0.3180)
predict(fit.d1, newdata, interval = "confidence")
```

```{r}
newdata = data.frame(density=0.2230)
predict(fit.d1, newdata, interval = "confidence")
```

```{r}
newdata = data.frame(density=0.1480)
predict(fit.d1, newdata, interval = "confidence")
```

```{r}
newdata = data.frame(density=0.0800)
predict(fit.d1, newdata, interval = "confidence")
```

```{r}
newdata = data.frame(density=0.0010)
predict(fit.d1, newdata, interval = "confidence")
```













